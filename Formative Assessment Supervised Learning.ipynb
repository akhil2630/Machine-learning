{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff49a258-58ee-44e5-aa95-9abe3d7cc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StandardScaler is used to normalize the features, which is important because:\n",
      " \n",
      "1.It ensures all features are on the same scale\n",
      "\n",
      "2.It improves the performance of algorithms like SVM and Logistic Regression\n",
      "\n",
      "3.It speeds up the convergence of many machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Loading and Preprocessing\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "#Explaination for the preprocessing\n",
    "print(\"\\nStandardScaler is used to normalize the features, which is important because:\")\n",
    "print( \" \\n1.It ensures all features are on the same scale\")\n",
    "print( \"\\n2.It improves the performance of algorithms like SVM and Logistic Regression\")\n",
    "print( \"\\n3.It speeds up the convergence of many machine learning algorithms\")\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1d4609-1314-4486-a7ea-c150c5f40e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression: A linear classifier that works well with numerical features\n"
     ]
    }
   ],
   "source": [
    "# 2. Classification Algorithm Implementation\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\nLogistic Regression: A linear classifier that works well with numerical features\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "results['Logistic Regression'] = accuracy_score(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd2cde05-5500-412f-8240-afc8005c3b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree: Creates a tree-like model of decisions, good for capturing non-linear patterns \n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "print(\"\\nDecision Tree: Creates a tree-like model of decisions, good for capturing non-linear patterns \")\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "dt_pred = dt_model.predict(X_test_scaled)\n",
    "results['Decision Tree'] = accuracy_score(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2247824-a44b-4a82-a7f5-2ed99698dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest: An ensemble of decision trees, generally provides better accuracy than a single tree \n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"\\nRandom Forest: An ensemble of decision trees, generally provides better accuracy than a single tree \")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "results['Random Forest'] = accuracy_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7270fd2-5c5e-4f6e-a153-0d4f9564ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM: Creates a hyperplane to separate classes, works well with high-dimensional data\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "print(\"\\nSVM: Creates a hyperplane to separate classes, works well with high-dimensional data\")\n",
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "results['SVM'] = accuracy_score(y_test, svm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3c0ed6-ead6-416e-9b30-9ff1014626cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k-Nearest Neighbours: Classifications based on nearest neighbors, good for non-linear patterns \n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "print(\"\\nk-Nearest Neighbours: Classifications based on nearest neighbors, good for non-linear patterns \")\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "results['k-NN'] = accuracy_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11f2ce0b-8add-4d38-a6b4-dba8da74546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of samples: 569\n",
      "Number of features: 30\n",
      "Feature names: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Comparison\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Algorithm': list(results.keys()),\n",
    "    'Accuracy': list(results.values())\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names: {data.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0870a88-ddc3-4fb4-8e87-5797a3f65fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison Results:\n",
      "             Algorithm  Accuracy\n",
      "3                  SVM  0.982456\n",
      "0  Logistic Regression  0.973684\n",
      "2        Random Forest  0.964912\n",
      "1        Decision Tree  0.947368\n",
      "4                 k-NN  0.947368\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Comparison Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0387bc4-963e-4a23-bcd5-6ba845870284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best model is :SVM\n",
      "\n",
      "The worst model is :k-NN\n",
      "\n",
      "Detailed Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        43\n",
      "           1       0.97      1.00      0.99        71\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.99      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print detailed report for the best performing model\n",
    "best_model = results_df.iloc[0]['Algorithm']\n",
    "worst_model = results_df.iloc[-1]['Algorithm']\n",
    "print(f\"\\nThe best model is :{best_model}\")\n",
    "print(f\"\\nThe worst model is :{worst_model}\")\n",
    "print(f\"\\nDetailed Classification Report for {best_model}:\")\n",
    "if best_model == 'Logistic Regression':\n",
    "    best_pred = lr_pred\n",
    "elif best_model == 'Decision Tree':\n",
    "    best_pred = dt_pred\n",
    "elif best_model == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "elif best_model == 'SVM':\n",
    "    best_pred = svm_pred\n",
    "else:\n",
    "    best_pred = knn_pred\n",
    "\n",
    "print(classification_report(y_test, best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7756cf9-d2d8-41ad-950e-17a625404408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
